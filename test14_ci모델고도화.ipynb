{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# AD 3D CNN - ROI Attention (í•´ë¶€í•™ì  ì¢Œí‘œ ê³ ì •)\n",
    "\n",
    "MCI vs AD ì´ì§„ë¶„ë¥˜ë¥¼ ìœ„í•œ 3D CNN ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "**ROI ì¢Œí‘œ ì„¤ì • ê°€ì´ë“œ:**\n",
    "- TARGET_SHAPE: (96, 112, 96) ê¸°ì¤€\n",
    "- RAS+ orientation (Right-Anterior-Superior positive)\n",
    "- Dì¶•(depth): Inferior(0) â†’ Superior(95)\n",
    "- Hì¶•(height): Posterior(0) â†’ Anterior(111)  \n",
    "- Wì¶•(width): Left(0) â†’ Right(95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {
    "executionInfo": {
     "elapsed": 5801,
     "status": "ok",
     "timestamp": 1771496528002,
     "user": {
      "displayName": "KDT7",
      "userId": "17761287236506147872"
     },
     "user_tz": -540
    },
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab_mount",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20708,
     "status": "ok",
     "timestamp": 1771496548708,
     "user": {
      "displayName": "KDT7",
      "userId": "17761287236506147872"
     },
     "user_tz": -540
    },
    "id": "colab_mount",
    "outputId": "c59ba0ca-f49a-423b-d9c0-80ecf71a14e9"
   },
   "outputs": [],
   "source": [
    "# Google Colab ì‚¬ìš© ì‹œ ì£¼ì„ í•´ì œ\n",
    "\"\"\"from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "    DATA_DIR = Path(\"/content/drive/MyDrive/7á„Œá…© á„Œá…¡á†¼á„‚á…¬á„‰á…¡á†·/á„á…¬á„Œá…©á†¼á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/mri_robust\")\n",
    "    METADATA_PATH = Path(\"/content/drive/MyDrive/7á„Œá…© á„Œá…¡á†¼á„‚á…¬á„‰á…¡á†·/á„á…¬á„Œá…©á†¼á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„ƒá…¦á„‹á…µá„á…¥á„‰á…¦á†º/idaSearch_image_download_metadata_2510.csv\")\n",
    "    OUTPUT_DIR = Path(\"/content/drive/MyDrive/7á„Œá…© á„Œá…¡á†¼á„‚á…¬á„‰á…¡á†·/á„á…¬á„Œá…©á†¼á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/á„†á…¡á„á…¡á†«á„’á…©/14\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {
    "id": "config_header"
   },
   "source": [
    "---\n",
    "## ğŸ”§ ì„¤ì • (Config)\n",
    "\n",
    "ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ê²½ë¡œë¥¼ ì—¬ê¸°ì„œ ê´€ë¦¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1771496548735,
     "user": {
      "displayName": "KDT7",
      "userId": "17761287236506147872"
     },
     "user_tz": -540
    },
    "id": "config",
    "outputId": "06fd75f1-3a14-40c2-e387-38d30827cbff"
   },
   "outputs": [],
   "source": [
    "basic = \"basic\"\n",
    "attention = \"attention\"\n",
    "transformer = \"transformer\"\n",
    "lstm = \"lstm\"\n",
    "gru = \"gru\"\n",
    "translstm = \"translstm\"\n",
    "transgru = \"transgru\"\n",
    "\n",
    "class Config:\n",
    "    # =====================================================\n",
    "    # ğŸ“ PATHS - ë°ì´í„° ê²½ë¡œ ì„¤ì •\n",
    "    # =====================================================\n",
    "    DATA_DIR = Path(\"/Users/machanho/Desktop/uv/ad/7.data_mci,ad\")\n",
    "    METADATA_PATH = Path(\"/Users/machanho/Desktop/uv/ad/idaSearch_image_download_metadata_2510.csv\")\n",
    "    OUTPUT_DIR = Path(\"/Users/machanho/Desktop/uv/ad/14\")\n",
    "\n",
    "    # =====================================================\n",
    "    # ğŸ² SEED / SPLIT\n",
    "    # =====================================================\n",
    "    SEED = 42\n",
    "    TEST_SIZE = 0.1\n",
    "    VAL_SIZE = 0.1\n",
    "\n",
    "    # =====================================================\n",
    "    # ğŸ“Š DATA\n",
    "    # =====================================================\n",
    "    TARGET_SHAPE = (96, 112, 96)  # (D, H, W)\n",
    "    NUM_CLASSES = 2\n",
    "    CLASS_NAMES = [\"MCI\", \"AD\"]\n",
    "\n",
    "    # =====================================================\n",
    "    # ğŸ‹ï¸ TRAINING\n",
    "    # =====================================================\n",
    "    BATCH_SIZE = 4\n",
    "    EPOCHS = 10\n",
    "    LR = 1e-4\n",
    "    GRAD_ACCUM_STEPS = 4  # effective batch = 16\n",
    "    EARLY_STOP_PATIENCE = 20\n",
    "    USE_WEIGHTED_SAMPLER = False\n",
    "\n",
    "    # =====================================================\n",
    "    # ğŸ§  MODEL\n",
    "    # =====================================================\n",
    "    MODEL_NAME = \"attention\"  # \"basic\" | \"attention\" | \"transformer\" | \"lstm\" | \"gru\" | \"translstm\" | \"transgru\"\n",
    "    DROPOUT = 0.25\n",
    "\n",
    "    # =====================================================\n",
    "    # ğŸ’» DEVICE\n",
    "    # =====================================================\n",
    "    @staticmethod\n",
    "    def get_device():\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "    DEVICE = None  # ì•„ë˜ì—ì„œ ì„¤ì •\n",
    "    NUM_WORKERS = 0\n",
    "\n",
    "Config.DEVICE = Config.get_device()\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "print(f\"Using device: {Config.DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roi_header",
   "metadata": {
    "id": "roi_header"
   },
   "source": [
    "---\n",
    "## ğŸ¯ ROI ì„¤ì • (Region of Interest)\n",
    "\n",
    "í•´ë¶€í•™ì  ìœ„ì¹˜ì— ê¸°ë°˜í•œ ROI ê°€ì¤‘ì¹˜ ì„¤ì •ì…ë‹ˆë‹¤.  \n",
    "**ì—¬ê¸°ì„œ ì¢Œí‘œë¥¼ ìˆ˜ì •**í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "### ì¢Œí‘œê³„ ì„¤ëª… (TARGET_SHAPE = 96 x 112 x 96)\n",
    "\n",
    "```\n",
    "RAS+ Orientation (MNI space ê¸°ì¤€ ì •ê·œí™” í›„)\n",
    "\n",
    "Dì¶• [0-95]:  Inferior(ì•„ë˜) â†’ Superior(ìœ„)     | ì¤‘ì•™: 48\n",
    "Hì¶• [0-111]: Posterior(ë’¤)  â†’ Anterior(ì•)    | ì¤‘ì•™: 56  \n",
    "Wì¶• [0-95]:  Left(ì¢Œ)       â†’ Right(ìš°)       | ì¤‘ì•™: 48\n",
    "```\n",
    "\n",
    "### ê°€ì¤‘ì¹˜ ê°€ì´ë“œ\n",
    "| ê°€ì¤‘ì¹˜ | ì˜ë¯¸ |\n",
    "|--------|------|\n",
    "| 1.0 | ê¸°ë³¸ (ê°•ì¡° ì—†ìŒ) |\n",
    "| 1.3 | ì•½ê°„ ê°•ì¡° |\n",
    "| 1.5 | ì¤‘ê°„ ê°•ì¡° |\n",
    "| 1.8 | ê°•í•œ ê°•ì¡° |\n",
    "| 2.0+ | ë§¤ìš° ê°•í•œ ê°•ì¡° |\n",
    "\n",
    "### MCI vs AD ì´ì§„ë¶„ë¥˜ ì°¸ê³ \n",
    "- **MCI (ê²½ë„ ì¸ì§€ì¥ì• )**: í•´ë§ˆ, ì—”í† ë¼ì´ë‚  í”¼ì§ˆ ì´ˆê¸° ë³€í™”\n",
    "- **AD (ì•Œì¸ í•˜ì´ë¨¸)**: ì¸¡ë‘ì—½, ë‘ì •ì—½ìœ¼ë¡œì˜ í™•ì¥ ìœ„ì¶• íŒ¨í„´\n",
    "- MCIì—ì„œ ADë¡œ ì§„í–‰ì‹œ ì „ì²´ì ì¸ ë‡Œ ìœ„ì¶• + ë‡Œì‹¤ í™•ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roi_config",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1771496548737,
     "user": {
      "displayName": "KDT7",
      "userId": "17761287236506147872"
     },
     "user_tz": -540
    },
    "id": "roi_config"
   },
   "outputs": [],
   "source": [
    "class ROIConfig:\n",
    "    \"\"\"\n",
    "    ğŸ¯ ROI ê°€ì¤‘ì¹˜ ì„¤ì •\n",
    "\n",
    "    ê° ROIì˜ centerëŠ” [D, H, W] ìˆœì„œì…ë‹ˆë‹¤.\n",
    "    sigma: Gaussian ë¶„í¬ì˜ í‘œì¤€í¸ì°¨ (í´ìˆ˜ë¡ ë„“ì€ ì˜ì—­ì— ì˜í–¥)\n",
    "    weight: í•´ë‹¹ ì˜ì—­ì˜ ê°€ì¤‘ì¹˜ ë°°ìˆ˜\n",
    "\n",
    "    ğŸ’¡ ìˆ˜ì • ë°©ë²•:\n",
    "    1. center ì¢Œí‘œ ì¡°ì •: ì˜ì—­ì˜ ì¤‘ì‹¬ ìœ„ì¹˜ ë³€ê²½\n",
    "    2. sigma ì¡°ì •: ì˜ì—­ì˜ í¬ê¸° ë³€ê²½\n",
    "    3. weight ì¡°ì •: í•´ë‹¹ ì˜ì—­ì˜ ì¤‘ìš”ë„ ë³€ê²½\n",
    "    4. ROI ì¶”ê°€/ì œê±°: ë”•ì…”ë„ˆë¦¬ì— í•­ëª© ì¶”ê°€/ì‚­ì œ\n",
    "    \"\"\"\n",
    "\n",
    "    # ROI Attention ì‚¬ìš© ì—¬ë¶€\n",
    "    USE_ROI_ATTENTION = True\n",
    "\n",
    "    # ê¸°ë³¸ ê°€ì¤‘ì¹˜ (ROI ì™¸ ì˜ì—­)\n",
    "    BASE_WEIGHT = 1.0\n",
    "\n",
    "    # =========================================================\n",
    "    # ğŸ§  ROI ì •ì˜\n",
    "    # =========================================================\n",
    "    # ì¢Œí‘œ: [D(Infâ†’Sup), H(Postâ†’Ant), W(Leftâ†’Right)]\n",
    "    # TARGET_SHAPE = (96, 112, 96) ê¸°ì¤€\n",
    "    # =========================================================\n",
    "\n",
    "    ROI_DEFINITIONS = {\n",
    "        # ===========================================================\n",
    "        # A. ë‚´ì¸¡ ì¸¡ë‘ì—½ (MTL) - MCI ì´ˆê¸° í•µì‹¬ ë°”ì´ì˜¤ë§ˆì»¤\n",
    "        # ===========================================================\n",
    "        # í•´ë§ˆ (Hippocampus)\n",
    "        \"hippocampus_L\": {\n",
    "            \"center\": [48, 54, 36],\n",
    "            \"sigma\": 4.0,\n",
    "            \"weight\": 2.0,\n",
    "            \"description\": \"ì¢Œì¸¡ í•´ë§ˆ - MCI/AD í•µì‹¬\"\n",
    "        },\n",
    "        \"hippocampus_R\": {\n",
    "            \"center\": [48, 54, 60],\n",
    "            \"sigma\": 4.0,\n",
    "            \"weight\": 2.0,\n",
    "            \"description\": \"ìš°ì¸¡ í•´ë§ˆ\"\n",
    "        },\n",
    "\n",
    "        # ì—”í† ë¼ì´ë‚  í”¼ì§ˆ (Entorhinal Cortex) - MCI ìµœì´ˆ ë³€í™”\n",
    "        \"entorhinal_L\": {\n",
    "            \"center\": [54, 48, 34],\n",
    "            \"sigma\": 3.5,\n",
    "            \"weight\": 2.0,\n",
    "            \"description\": \"ì¢Œì¸¡ ì—”í† ë¼ì´ë‚  - MCI ì´ˆê¸°\"\n",
    "        },\n",
    "        \"entorhinal_R\": {\n",
    "            \"center\": [54, 48, 62],\n",
    "            \"sigma\": 3.5,\n",
    "            \"weight\": 2.0,\n",
    "            \"description\": \"ìš°ì¸¡ ì—”í† ë¼ì´ë‚ \"\n",
    "        },\n",
    "\n",
    "        # í•´ë§ˆë°©íšŒ (Parahippocampal Gyrus)\n",
    "        \"parahippocampal_L\": {\n",
    "            \"center\": [48, 42, 32],\n",
    "            \"sigma\": 4.0,\n",
    "            \"weight\": 1.6,\n",
    "            \"description\": \"ì¢Œì¸¡ í•´ë§ˆë°©íšŒ\"\n",
    "        },\n",
    "        \"parahippocampal_R\": {\n",
    "            \"center\": [48, 42, 64],\n",
    "            \"sigma\": 4.0,\n",
    "            \"weight\": 1.6,\n",
    "            \"description\": \"ìš°ì¸¡ í•´ë§ˆë°©íšŒ\"\n",
    "        },\n",
    "\n",
    "        # í¸ë„ì²´ (Amygdala)\n",
    "        \"amygdala_L\": {\n",
    "            \"center\": [42, 60, 34],\n",
    "            \"sigma\": 3.5,\n",
    "            \"weight\": 1.5,\n",
    "            \"description\": \"ì¢Œì¸¡ í¸ë„ì²´\"\n",
    "        },\n",
    "        \"amygdala_R\": {\n",
    "            \"center\": [42, 60, 62],\n",
    "            \"sigma\": 3.5,\n",
    "            \"weight\": 1.5,\n",
    "            \"description\": \"ìš°ì¸¡ í¸ë„ì²´\"\n",
    "        },\n",
    "\n",
    "        # ===========================================================\n",
    "        # B. AD signature + ì—°ê´€ í”¼ì§ˆ ì˜ì—­ (í™•ì¥ì„± ìœ„ì¶•)\n",
    "        # ===========================================================\n",
    "        # í›„ëŒ€ìƒí”¼ì§ˆ (Posterior Cingulate Cortex)\n",
    "        \"posterior_cingulate\": {\n",
    "            \"center\": [30, 35, 48],\n",
    "            \"sigma\": 5.0,\n",
    "            \"weight\": 1.4,\n",
    "            \"description\": \"í›„ëŒ€ìƒí”¼ì§ˆ - AD í™•ì¥ íŒ¨í„´\"\n",
    "        },\n",
    "\n",
    "        # ìê¸°ì•ì†Œì—½ (Precuneus)\n",
    "        \"precuneus\": {\n",
    "            \"center\": [65, 30, 48],\n",
    "            \"sigma\": 6.0,\n",
    "            \"weight\": 1.4,\n",
    "            \"description\": \"precuneus - AD í™•ì¥ íŒ¨í„´\"\n",
    "        },\n",
    "\n",
    "        # Inferior / Middle Temporal + Fusiform (AD signature)\n",
    "        \"inferior_temporal_L\": {\n",
    "            \"center\": [42, 60, 22],\n",
    "            \"sigma\": 8.0,\n",
    "            \"weight\": 1.3,\n",
    "            \"description\": \"ì¢Œì¸¡ í•˜ì¸¡ë‘íšŒ\"\n",
    "        },\n",
    "        \"inferior_temporal_R\": {\n",
    "            \"center\": [42, 60, 72],\n",
    "            \"sigma\": 8.0,\n",
    "            \"weight\": 1.3,\n",
    "            \"description\": \"ìš°ì¸¡ í•˜ì¸¡ë‘íšŒ\"\n",
    "        },\n",
    "        \"middle_temporal_L\": {\n",
    "            \"center\": [48, 52, 24],\n",
    "            \"sigma\": 8.0,\n",
    "            \"weight\": 1.3,\n",
    "            \"description\": \"ì¢Œì¸¡ ì¤‘ì¸¡ë‘íšŒ\"\n",
    "        },\n",
    "        \"middle_temporal_R\": {\n",
    "            \"center\": [48, 52, 72],\n",
    "            \"sigma\": 8.0,\n",
    "            \"weight\": 1.3,\n",
    "            \"description\": \"ìš°ì¸¡ ì¤‘ì¸¡ë‘íšŒ\"\n",
    "        },\n",
    "        \"fusiform_L\": {\n",
    "            \"center\": [38, 58, 28],\n",
    "            \"sigma\": 6.0,\n",
    "            \"weight\": 1.3,\n",
    "            \"description\": \"ì¢Œì¸¡ ë°©ì¶”ìƒíšŒ\"\n",
    "        },\n",
    "        \"fusiform_R\": {\n",
    "            \"center\": [38, 58, 68],\n",
    "            \"sigma\": 6.0,\n",
    "            \"weight\": 1.3,\n",
    "            \"description\": \"ìš°ì¸¡ ë°©ì¶”ìƒíšŒ\"\n",
    "        },\n",
    "\n",
    "        # ë‘ì •ì—½ (Parietal Cortex)\n",
    "        \"parietal_L\": {\n",
    "            \"center\": [68, 45, 26],\n",
    "            \"sigma\": 8.0,\n",
    "            \"weight\": 1.2,\n",
    "            \"description\": \"ì¢Œì¸¡ ë‘ì •ì—½ - temporo-parietal í™•ì¥\"\n",
    "        },\n",
    "        \"parietal_R\": {\n",
    "            \"center\": [68, 45, 70],\n",
    "            \"sigma\": 8.0,\n",
    "            \"weight\": 1.2,\n",
    "            \"description\": \"ìš°ì¸¡ ë‘ì •ì—½\"\n",
    "        },\n",
    "\n",
    "        # ì „ë‘ì—½ (Frontal Cortex) - í›„ê¸° í™•ì¥\n",
    "        \"frontal\": {\n",
    "            \"center\": [55, 85, 48],\n",
    "            \"sigma\": 10.0,\n",
    "            \"weight\": 1.1,\n",
    "            \"description\": \"ì „ë‘ì—½ - í›„ê¸° í™•ì¥\"\n",
    "        },\n",
    "\n",
    "        # ===========================================================\n",
    "        # C. ë‡Œì‹¤ (Ventricles) - ìœ„ì¶• ë³´ìƒ ì§€í‘œ\n",
    "        # ===========================================================\n",
    "        \"lateral_ventricle\": {\n",
    "            \"center\": [35, 56, 48],\n",
    "            \"sigma\": 12.0,\n",
    "            \"weight\": 1.5,\n",
    "            \"description\": \"ì¸¡ë‡Œì‹¤ - ìœ„ì¶• ë³´ìƒ í™•ì¥\"\n",
    "        },\n",
    "        \"inferior_horn_L\": {\n",
    "            \"center\": [35, 56, 30],\n",
    "            \"sigma\": 8.0,\n",
    "            \"weight\": 1.5,\n",
    "            \"description\": \"ì¢Œì¸¡ í•˜ê° - í•´ë§ˆ ìœ„ì¶• ì§€í‘œ\"\n",
    "        },\n",
    "        \"inferior_horn_R\": {\n",
    "            \"center\": [35, 56, 66],\n",
    "            \"sigma\": 8.0,\n",
    "            \"weight\": 1.5,\n",
    "            \"description\": \"ìš°ì¸¡ í•˜ê°\"\n",
    "        },\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_roi_summary(cls):\n",
    "        \"\"\"ROI ì„¤ì • ìš”ì•½ ì¶œë ¥\"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"ğŸ¯ ROI Configuration Summary\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"USE_ROI_ATTENTION: {cls.USE_ROI_ATTENTION}\")\n",
    "        print(f\"BASE_WEIGHT: {cls.BASE_WEIGHT}\")\n",
    "        print(f\"Number of ROIs: {len(cls.ROI_DEFINITIONS)}\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"{'Name':<25} {'Center [D,H,W]':<20} {'Ïƒ':<8} {'Weight':<8}\")\n",
    "        print(\"-\" * 70)\n",
    "        for name, roi in cls.ROI_DEFINITIONS.items():\n",
    "            center_str = str(roi['center'])\n",
    "            print(f\"{name:<25} {center_str:<20} {roi['sigma']:<8.1f} {roi['weight']:<8.2f}\")\n",
    "        print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "print_config",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1771496548744,
     "user": {
      "displayName": "KDT7",
      "userId": "17761287236506147872"
     },
     "user_tz": -540
    },
    "id": "print_config",
    "outputId": "e9877c32-8141-4f2b-e569-ddd5c7fcc31a"
   },
   "outputs": [],
   "source": [
    "# ì„¤ì • í™•ì¸\n",
    "print(\"=\" * 70)\n",
    "print(\"CONFIGURATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Device: {Config.DEVICE}\")\n",
    "print(f\"Model: {Config.MODEL_NAME}\")\n",
    "print(f\"Target shape: {Config.TARGET_SHAPE}\")\n",
    "print(f\"Classes: {Config.CLASS_NAMES} (2-class classification: MCI vs AD)\")\n",
    "print(f\"Batch size: {Config.BATCH_SIZE} x {Config.GRAD_ACCUM_STEPS} = {Config.BATCH_SIZE * Config.GRAD_ACCUM_STEPS} (effective)\")\n",
    "print(f\"Output dir: {Config.OUTPUT_DIR}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ROIConfig.get_roi_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_header",
   "metadata": {
    "id": "data_header"
   },
   "source": [
    "---\n",
    "## ğŸ“Š ë°ì´í„° ì¸ë±ì‹± / ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_indexing",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1771496548748,
     "user": {
      "displayName": "KDT7",
      "userId": "17761287236506147872"
     },
     "user_tz": -540
    },
    "id": "data_indexing"
   },
   "outputs": [],
   "source": [
    "_SUBJECT_RE = re.compile(r\"__([0-9]{3}_S_[0-9]+)__\")\n",
    "_SUBJECT_FALLBACK_RE = re.compile(r\"([0-9]{3}_S_[0-9]+)\")\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SplitConfig:\n",
    "    seed: int = Config.SEED\n",
    "    test_size: float = Config.TEST_SIZE\n",
    "    val_size: float = Config.VAL_SIZE\n",
    "\n",
    "\n",
    "def _extract_subject_id(name: str) -> str | None:\n",
    "    match = _SUBJECT_RE.search(name)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    match = _SUBJECT_FALLBACK_RE.search(name)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "def build_index(\n",
    "    data_dir: Path,\n",
    "    metadata_csv: Path,\n",
    ") -> pd.DataFrame:\n",
    "    if not data_dir.exists():\n",
    "        raise FileNotFoundError(f\"data_dir not found: {data_dir}\")\n",
    "    if not metadata_csv.exists():\n",
    "        raise FileNotFoundError(f\"metadata_csv not found: {metadata_csv}\")\n",
    "\n",
    "    meta = pd.read_csv(metadata_csv)\n",
    "    meta = meta[[\"Subject ID\", \"Research Group\"]].dropna()\n",
    "    meta = meta.drop_duplicates(subset=[\"Subject ID\"], keep=\"last\")\n",
    "\n",
    "    group_map = {\"MCI\": 0, \"AD\": 1}  # CN ë°ì´í„°ëŠ” ì œì™¸\n",
    "\n",
    "    rows = []\n",
    "    for path in sorted(data_dir.glob(\"*.nii*\")):\n",
    "        subject_id = _extract_subject_id(path.name)\n",
    "        if not subject_id:\n",
    "            continue\n",
    "        row = meta.loc[meta[\"Subject ID\"] == subject_id]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        group = row.iloc[0][\"Research Group\"]\n",
    "        if group not in group_map:\n",
    "            continue\n",
    "        label = group_map[group]\n",
    "        rows.append(\n",
    "            {\n",
    "                \"path\": str(path),\n",
    "                \"subject_id\": subject_id,\n",
    "                \"group\": group,\n",
    "                \"label\": label,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"No matching files found for metadata and data_dir.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_by_subject(df: pd.DataFrame, config: SplitConfig) -> dict[str, pd.DataFrame]:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    subject_df = df.drop_duplicates(subset=[\"subject_id\"])[[\"subject_id\", \"label\"]]\n",
    "    subjects = subject_df[\"subject_id\"].tolist()\n",
    "    labels = subject_df[\"label\"].tolist()\n",
    "\n",
    "    train_subjects, test_subjects = train_test_split(\n",
    "        subjects,\n",
    "        test_size=config.test_size,\n",
    "        random_state=config.seed,\n",
    "        stratify=labels,\n",
    "    )\n",
    "\n",
    "    train_labels = [\n",
    "        subject_df.loc[subject_df[\"subject_id\"] == sid, \"label\"].iloc[0]\n",
    "        for sid in train_subjects\n",
    "    ]\n",
    "    train_subjects, val_subjects = train_test_split(\n",
    "        train_subjects,\n",
    "        test_size=config.val_size,\n",
    "        random_state=config.seed,\n",
    "        stratify=train_labels,\n",
    "    )\n",
    "\n",
    "    def _select(subject_list: list[str]) -> pd.DataFrame:\n",
    "        return df[df[\"subject_id\"].isin(subject_list)].reset_index(drop=True)\n",
    "\n",
    "    return {\n",
    "        \"train\": _select(train_subjects),\n",
    "        \"val\": _select(val_subjects),\n",
    "        \"test\": _select(test_subjects),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_header",
   "metadata": {
    "id": "dataset_header"
   },
   "source": [
    "---\n",
    "## ğŸ“¦ Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset",
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1771496548779,
     "user": {
      "displayName": "KDT7",
      "userId": "17761287236506147872"
     },
     "user_tz": -540
    },
    "id": "dataset"
   },
   "outputs": [],
   "source": [
    "def _normalize(volume: np.ndarray) -> np.ndarray:\n",
    "    v = volume.astype(np.float32)\n",
    "    lo, hi = np.percentile(v, (1, 99))\n",
    "    if hi <= lo:\n",
    "        lo, hi = v.min(), v.max()\n",
    "    if hi <= lo:\n",
    "        return np.zeros_like(v, dtype=np.float32)\n",
    "    v = np.clip((v - lo) / (hi - lo), 0, 1)\n",
    "    return v.astype(np.float32)\n",
    "\n",
    "\n",
    "def _resize_tensor(volume: torch.Tensor, target_shape: tuple[int, int, int]) -> torch.Tensor:\n",
    "    vol = volume.unsqueeze(0).unsqueeze(0)  # (1, 1, D, H, W)\n",
    "    vol = F.interpolate(vol, size=target_shape, mode=\"trilinear\", align_corners=False)\n",
    "    return vol.squeeze(0)  # (1, D, H, W)\n",
    "\n",
    "\n",
    "class NiftiDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        target_shape: tuple[int, int, int] = (96, 112, 96),\n",
    "        augment: bool = False,\n",
    "    ) -> None:\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.target_shape = target_shape\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def _augment(self, volume: torch.Tensor) -> torch.Tensor:\n",
    "        # Intensity augmentation\n",
    "        if torch.rand(1).item() < 0.3:\n",
    "            volume = volume * (0.9 + 0.2 * torch.rand(1).item())  # brightness\n",
    "        if torch.rand(1).item() < 0.3:\n",
    "            volume = volume + (torch.rand(1).item() - 0.5) * 0.1  # shift\n",
    "        volume = torch.clamp(volume, 0, 1)\n",
    "        return volume\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, int]:\n",
    "        import nibabel as nib\n",
    "\n",
    "        row = self.df.iloc[idx]\n",
    "        path = row[\"path\"]\n",
    "        try:\n",
    "            img = nib.load(path)\n",
    "            data = img.get_fdata()\n",
    "            if data.ndim == 4:\n",
    "                data = data[..., 0]\n",
    "            if data.ndim != 3:\n",
    "                raise ValueError(f\"Unexpected NIfTI shape {data.shape}\")\n",
    "            data = _normalize(data)\n",
    "            vol = torch.from_numpy(data)\n",
    "            vol = _resize_tensor(vol, self.target_shape)\n",
    "        except Exception as exc:\n",
    "            raise RuntimeError(f\"Failed to load/prepare NIfTI: {path}\") from exc\n",
    "        if self.augment:\n",
    "            vol = self._augment(vol)\n",
    "        label = int(row[\"label\"])\n",
    "        return vol, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_header",
   "metadata": {
    "id": "model_header"
   },
   "source": [
    "---\n",
    "## ğŸ§  ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roi_attention_module",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1771496548792,
     "user": {
      "displayName": "KDT7",
      "userId": "17761287236506147872"
     },
     "user_tz": -540
    },
    "id": "roi_attention_module"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ROI Spatial Attention Module\n",
    "# ============================================================\n",
    "\n",
    "def create_roi_weight_map(\n",
    "    target_shape: tuple[int, int, int],\n",
    "    roi_definitions: dict,\n",
    "    base_weight: float = ROIConfig.BASE_WEIGHT\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    ROI ê¸°ë°˜ 3D ê°€ì¤‘ì¹˜ ë§µ ìƒì„±\n",
    "\n",
    "    ê° ROIëŠ” 3D Gaussianìœ¼ë¡œ ëª¨ë¸ë§ë˜ë©°, ì¤‘ì²© ì˜ì—­ì€ ê°€ì¤‘ì¹˜ê°€ í•©ì‚°ë©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    D, H, W = target_shape\n",
    "    weight_map = torch.ones(1, 1, D, H, W) * base_weight\n",
    "\n",
    "    d = torch.arange(D).float()\n",
    "    h = torch.arange(H).float()\n",
    "    w = torch.arange(W).float()\n",
    "    dd, hh, ww = torch.meshgrid(d, h, w, indexing='ij')\n",
    "\n",
    "    for name, roi in roi_definitions.items():\n",
    "        center = roi[\"center\"]\n",
    "        sigma = roi[\"sigma\"]\n",
    "        roi_weight = roi[\"weight\"]\n",
    "\n",
    "        dist_sq = ((dd - center[0])**2 + (hh - center[1])**2 + (ww - center[2])**2)\n",
    "        gaussian = torch.exp(-dist_sq / (2 * sigma**2))\n",
    "        weight_map[0, 0] += gaussian * max(0.0, roi_weight - base_weight)\n",
    "\n",
    "    return weight_map\n",
    "\n",
    "\n",
    "class ROISpatialAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    ì…ë ¥ ë³¼ë¥¨ì— ROI ê¸°ë°˜ ê³µê°„ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•˜ëŠ” ëª¨ë“ˆ\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_shape: tuple[int, int, int],\n",
    "        roi_definitions: dict,\n",
    "        base_weight: float = ROIConfig.BASE_WEIGHT\n",
    "    ):\n",
    "        super().__init__()\n",
    "        weight_map = create_roi_weight_map(target_shape, roi_definitions, base_weight)\n",
    "        self.register_buffer('weight_map', weight_map)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * self.weight_map\n",
    "\n",
    "\n",
    "def build_roi_attention(config: type, roi_config: type) -> ROISpatialAttention | None:\n",
    "    \"\"\"\n",
    "    ROI Attention ëª¨ë“ˆ ìƒì„±\n",
    "    \"\"\"\n",
    "    if not roi_config.USE_ROI_ATTENTION:\n",
    "        return None\n",
    "\n",
    "    return ROISpatialAttention(\n",
    "        target_shape=config.TARGET_SHAPE,\n",
    "        roi_definitions=roi_config.ROI_DEFINITIONS,\n",
    "        base_weight=roi_config.BASE_WEIGHT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_blocks",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1771496548793,
     "user": {
      "displayName": "KDT7",
      "userId": "17761287236506147872"
     },
     "user_tz": -540
    },
    "id": "model_blocks"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Model Building Blocks\n",
    "# ============================================================\n",
    "\n",
    "class ConvBlock3D(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int) -> None:\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels: int, reduction: int = 8) -> None:\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, c, _, _, _ = x.shape\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1, 1)\n",
    "        return x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "models",
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1771496548819,
     "user": {
      "displayName": "KDT7",
      "userId": "17761287236506147872"
     },
     "user_tz": -540
    },
    "id": "models"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# Models\n",
    "# ============================================================\n",
    "\n",
    "class MCIClassifier3D(nn.Module):\n",
    "    \"\"\"Basic 3D CNN for MCI classification\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = Config.NUM_CLASSES,\n",
    "        dropout: float = Config.DROPOUT,\n",
    "        roi_attention: nn.Module | None = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.roi_attention = roi_attention\n",
    "        self.encoder = nn.Sequential(\n",
    "            ConvBlock3D(1, 32),\n",
    "            ConvBlock3D(32, 64),\n",
    "            ConvBlock3D(64, 128),\n",
    "            ConvBlock3D(128, 256),\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.roi_attention is not None:\n",
    "            x = self.roi_attention(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.pool(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class MCIClassifierWithAttention(nn.Module):\n",
    "    \"\"\"3D CNN with Channel Attention\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = Config.NUM_CLASSES,\n",
    "        dropout: float = Config.DROPOUT,\n",
    "        roi_attention: nn.Module | None = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.roi_attention = roi_attention\n",
    "        self.block1 = ConvBlock3D(1, 32)\n",
    "        self.block2 = ConvBlock3D(32, 64)\n",
    "        self.block3 = ConvBlock3D(64, 128)\n",
    "        self.block4 = ConvBlock3D(128, 256)\n",
    "        self.attn = ChannelAttention(256)\n",
    "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.roi_attention is not None:\n",
    "            x = self.roi_attention(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.pool(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "class MCIClassifierCNNTransformer(nn.Module):\n",
    "    \"\"\"3D CNN encoder + Transformer encoder classifier\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = Config.NUM_CLASSES,\n",
    "        dropout: float = Config.DROPOUT,\n",
    "        roi_attention: nn.Module | None = None,\n",
    "        embed_dim: int = 256,\n",
    "        num_heads: int = 8,\n",
    "        num_layers: int = 2,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.roi_attention = roi_attention\n",
    "        self.encoder = nn.Sequential(\n",
    "            ConvBlock3D(1, 32),\n",
    "            ConvBlock3D(32, 64),\n",
    "            ConvBlock3D(64, 128),\n",
    "            ConvBlock3D(128, embed_dim),\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _sinusoidal_positional_encoding(\n",
    "        seq_len: int,\n",
    "        embed_dim: int,\n",
    "        device: torch.device,\n",
    "        dtype: torch.dtype,\n",
    "    ) -> torch.Tensor:\n",
    "        position = torch.arange(seq_len, device=device, dtype=dtype).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embed_dim, 2, device=device, dtype=dtype)\n",
    "            * (-torch.log(torch.tensor(10000.0, device=device, dtype=dtype)) / embed_dim)\n",
    "        )\n",
    "        pe = torch.zeros(seq_len, embed_dim, device=device, dtype=dtype)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.roi_attention is not None:\n",
    "            x = self.roi_attention(x)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        b, c, d, h, w = x.shape\n",
    "\n",
    "        tokens = x.flatten(2).transpose(1, 2)  # [B, D*H*W, C]\n",
    "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
    "        tokens = torch.cat([cls_tokens, tokens], dim=1)\n",
    "\n",
    "        pos = self._sinusoidal_positional_encoding(\n",
    "            seq_len=tokens.size(1),\n",
    "            embed_dim=tokens.size(2),\n",
    "            device=tokens.device,\n",
    "            dtype=tokens.dtype,\n",
    "        )\n",
    "        tokens = self.dropout(tokens + pos.unsqueeze(0))\n",
    "\n",
    "        encoded = self.transformer(tokens)\n",
    "        cls_repr = self.norm(encoded[:, 0])\n",
    "        return self.classifier(cls_repr)\n",
    "\n",
    "\n",
    "\n",
    "class MCIClassifierTransLSTM(nn.Module):\n",
    "    \"\"\"3D CNN encoder + Transformer + LSTM classifier\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = Config.NUM_CLASSES,\n",
    "        dropout: float = Config.DROPOUT,\n",
    "        roi_attention: nn.Module | None = None,\n",
    "        embed_dim: int = 256,\n",
    "        num_heads: int = 8,\n",
    "        num_layers: int = 2,\n",
    "        lstm_hidden_dim: int = 128,\n",
    "        lstm_layers: int = 2,\n",
    "        bidirectional: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.roi_attention = roi_attention\n",
    "        self.bidirectional = bidirectional\n",
    "        self.encoder = nn.Sequential(\n",
    "            ConvBlock3D(1, 32),\n",
    "            ConvBlock3D(32, 64),\n",
    "            ConvBlock3D(64, 128),\n",
    "            ConvBlock3D(128, embed_dim),\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            dropout=dropout if lstm_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        lstm_out_dim = lstm_hidden_dim * (2 if bidirectional else 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_out_dim, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _sinusoidal_positional_encoding(\n",
    "        seq_len: int,\n",
    "        embed_dim: int,\n",
    "        device: torch.device,\n",
    "        dtype: torch.dtype,\n",
    "    ) -> torch.Tensor:\n",
    "        position = torch.arange(seq_len, device=device, dtype=dtype).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embed_dim, 2, device=device, dtype=dtype)\n",
    "            * (-torch.log(torch.tensor(10000.0, device=device, dtype=dtype)) / embed_dim)\n",
    "        )\n",
    "        pe = torch.zeros(seq_len, embed_dim, device=device, dtype=dtype)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.roi_attention is not None:\n",
    "            x = self.roi_attention(x)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        seq = x.mean(dim=(3, 4)).transpose(1, 2)  # [B, D, C]\n",
    "        pos = self._sinusoidal_positional_encoding(\n",
    "            seq_len=seq.size(1),\n",
    "            embed_dim=seq.size(2),\n",
    "            device=seq.device,\n",
    "            dtype=seq.dtype,\n",
    "        )\n",
    "        seq = self.dropout(seq + pos.unsqueeze(0))\n",
    "        encoded = self.transformer(seq)\n",
    "        encoded = self.norm(encoded)\n",
    "\n",
    "        _, (h_n, _) = self.lstm(encoded)\n",
    "        if self.bidirectional:\n",
    "            features = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
    "        else:\n",
    "            features = h_n[-1]\n",
    "\n",
    "        features = self.dropout(features)\n",
    "        return self.classifier(features)\n",
    "\n",
    "\n",
    "class MCIClassifierTransGRU(nn.Module):\n",
    "    \"\"\"3D CNN encoder + Transformer + GRU classifier\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = Config.NUM_CLASSES,\n",
    "        dropout: float = Config.DROPOUT,\n",
    "        roi_attention: nn.Module | None = None,\n",
    "        embed_dim: int = 256,\n",
    "        num_heads: int = 8,\n",
    "        num_layers: int = 2,\n",
    "        gru_hidden_dim: int = 128,\n",
    "        gru_layers: int = 2,\n",
    "        bidirectional: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.roi_attention = roi_attention\n",
    "        self.bidirectional = bidirectional\n",
    "        self.encoder = nn.Sequential(\n",
    "            ConvBlock3D(1, 32),\n",
    "            ConvBlock3D(32, 64),\n",
    "            ConvBlock3D(64, 128),\n",
    "            ConvBlock3D(128, embed_dim),\n",
    "        )\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=embed_dim * 4,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=gru_hidden_dim,\n",
    "            num_layers=gru_layers,\n",
    "            dropout=dropout if gru_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        gru_out_dim = gru_hidden_dim * (2 if bidirectional else 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(gru_out_dim, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _sinusoidal_positional_encoding(\n",
    "        seq_len: int,\n",
    "        embed_dim: int,\n",
    "        device: torch.device,\n",
    "        dtype: torch.dtype,\n",
    "    ) -> torch.Tensor:\n",
    "        position = torch.arange(seq_len, device=device, dtype=dtype).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embed_dim, 2, device=device, dtype=dtype)\n",
    "            * (-torch.log(torch.tensor(10000.0, device=device, dtype=dtype)) / embed_dim)\n",
    "        )\n",
    "        pe = torch.zeros(seq_len, embed_dim, device=device, dtype=dtype)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.roi_attention is not None:\n",
    "            x = self.roi_attention(x)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        seq = x.mean(dim=(3, 4)).transpose(1, 2)  # [B, D, C]\n",
    "        pos = self._sinusoidal_positional_encoding(\n",
    "            seq_len=seq.size(1),\n",
    "            embed_dim=seq.size(2),\n",
    "            device=seq.device,\n",
    "            dtype=seq.dtype,\n",
    "        )\n",
    "        seq = self.dropout(seq + pos.unsqueeze(0))\n",
    "        encoded = self.transformer(seq)\n",
    "        encoded = self.norm(encoded)\n",
    "\n",
    "        _, h_n = self.gru(encoded)\n",
    "        if self.bidirectional:\n",
    "            features = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
    "        else:\n",
    "            features = h_n[-1]\n",
    "\n",
    "        features = self.dropout(features)\n",
    "        return self.classifier(features)\n",
    "\n",
    "class MCIClassifierCNNLSTM(nn.Module):\n",
    "    \"\"\"3D CNN encoder + LSTM classifier\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = Config.NUM_CLASSES,\n",
    "        dropout: float = Config.DROPOUT,\n",
    "        roi_attention: nn.Module | None = None,\n",
    "        embed_dim: int = 256,\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        bidirectional: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.roi_attention = roi_attention\n",
    "        self.bidirectional = bidirectional\n",
    "        self.encoder = nn.Sequential(\n",
    "            ConvBlock3D(1, 32),\n",
    "            ConvBlock3D(32, 64),\n",
    "            ConvBlock3D(64, 128),\n",
    "            ConvBlock3D(128, embed_dim),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        lstm_out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_out_dim, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.roi_attention is not None:\n",
    "            x = self.roi_attention(x)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        seq = x.mean(dim=(3, 4)).transpose(1, 2)  # [B, D, C]\n",
    "\n",
    "        _, (h_n, _) = self.lstm(seq)\n",
    "        if self.bidirectional:\n",
    "            features = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
    "        else:\n",
    "            features = h_n[-1]\n",
    "\n",
    "        features = self.dropout(features)\n",
    "        return self.classifier(features)\n",
    "\n",
    "\n",
    "class MCIClassifierCNNGRU(nn.Module):\n",
    "    \"\"\"3D CNN encoder + GRU classifier\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = Config.NUM_CLASSES,\n",
    "        dropout: float = Config.DROPOUT,\n",
    "        roi_attention: nn.Module | None = None,\n",
    "        embed_dim: int = 256,\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        bidirectional: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.roi_attention = roi_attention\n",
    "        self.bidirectional = bidirectional\n",
    "        self.encoder = nn.Sequential(\n",
    "            ConvBlock3D(1, 32),\n",
    "            ConvBlock3D(32, 64),\n",
    "            ConvBlock3D(64, 128),\n",
    "            ConvBlock3D(128, embed_dim),\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        gru_out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(gru_out_dim, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.roi_attention is not None:\n",
    "            x = self.roi_attention(x)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        seq = x.mean(dim=(3, 4)).transpose(1, 2)  # [B, D, C]\n",
    "\n",
    "        _, h_n = self.gru(seq)\n",
    "        if self.bidirectional:\n",
    "            features = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
    "        else:\n",
    "            features = h_n[-1]\n",
    "\n",
    "        features = self.dropout(features)\n",
    "        return self.classifier(features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_header",
   "metadata": {
    "id": "training_header"
   },
   "source": [
    "---\n",
    "## ğŸ‹ï¸ í•™ìŠµ / í‰ê°€ ë£¨í‹´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1771496548868,
     "user": {
      "displayName": "KDT7",
      "userId": "17761287236506147872"
     },
     "user_tz": -540
    },
    "id": "training"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def build_dataloaders(\n",
    "    splits: dict[str, pd.DataFrame],\n",
    "    target_shape: tuple[int, int, int],\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    use_weighted_sampler: bool,\n",
    ") -> dict[str, DataLoader]:\n",
    "    train_ds = NiftiDataset(splits[\"train\"], target_shape=target_shape, augment=True)\n",
    "    val_ds = NiftiDataset(splits[\"val\"], target_shape=target_shape, augment=False)\n",
    "    test_ds = NiftiDataset(splits[\"test\"], target_shape=target_shape, augment=False)\n",
    "\n",
    "    if use_weighted_sampler:\n",
    "        labels = splits[\"train\"][\"label\"].tolist()\n",
    "        class_counts = np.bincount(labels, minlength=Config.NUM_CLASSES)\n",
    "        class_weights = 1.0 / np.maximum(class_counts, 1)\n",
    "        sample_weights = [class_weights[label] for label in labels]\n",
    "        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "        shuffle = False\n",
    "    else:\n",
    "        sampler = None\n",
    "        shuffle = True\n",
    "\n",
    "    return {\n",
    "        \"train\": DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        ),\n",
    "        \"val\": DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        ),\n",
    "        \"test\": DataLoader(\n",
    "            test_ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device, num_classes: int = Config.NUM_CLASSES) -> dict[str, float]:\n",
    "    model.eval()\n",
    "    all_labels: list[int] = []\n",
    "    all_probs: list[list[float]] = []\n",
    "    all_preds: list[int] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for vols, labels in loader:\n",
    "            vols = vols.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(vols)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "            all_probs.extend(probs.cpu().numpy().tolist())\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "\n",
    "    all_probs_np = np.array(all_probs)\n",
    "    all_labels_np = np.array(all_labels)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(all_labels, all_preds),\n",
    "        \"precision_macro\": precision_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "        \"recall_macro\": recall_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "        \"f1_macro\": f1_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "        \"f1_weighted\": f1_score(all_labels, all_preds, average=\"weighted\", zero_division=0),\n",
    "    }\n",
    "\n",
    "    # Per-class metrics\n",
    "    for i, name in enumerate(Config.CLASS_NAMES):\n",
    "        metrics[f\"precision_{name}\"] = precision_score(all_labels, all_preds, labels=[i], average=\"micro\", zero_division=0)\n",
    "        metrics[f\"recall_{name}\"] = recall_score(all_labels, all_preds, labels=[i], average=\"micro\", zero_division=0)\n",
    "        metrics[f\"f1_{name}\"] = f1_score(all_labels, all_preds, labels=[i], average=\"micro\", zero_division=0)\n",
    "\n",
    "    # AUC (one-vs-rest)\n",
    "    try:\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        labels_bin = label_binarize(all_labels_np, classes=list(range(num_classes)))\n",
    "        metrics[\"auc_macro\"] = roc_auc_score(labels_bin, all_probs_np, average=\"macro\", multi_class=\"ovr\")\n",
    "    except ValueError:\n",
    "        metrics[\"auc_macro\"] = float(\"nan\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_confusion_matrix(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    num_classes: int = Config.NUM_CLASSES\n",
    ") -> np.ndarray:\n",
    "    model.eval()\n",
    "    all_labels: list[int] = []\n",
    "    all_preds: list[int] = []\n",
    "    with torch.no_grad():\n",
    "        for vols, labels in loader:\n",
    "            vols = vols.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(vols)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "    return confusion_matrix(all_labels, all_preds, labels=list(range(num_classes)))\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    loaders: dict[str, DataLoader],\n",
    "    device: torch.device,\n",
    "    epochs: int,\n",
    "    lr: float,\n",
    "    output_dir: Path,\n",
    "    early_stop_patience: int,\n",
    "    grad_accum_steps: int,\n",
    "    num_classes: int,\n",
    ") -> dict[str, list]:\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    history: dict[str, list] = {\"train\": [], \"val\": [], \"val_confusion\": []}\n",
    "    best_f1 = -1.0\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_losses: list[float] = []\n",
    "        grad_norms: list[float] = []\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        batch_iter = tqdm(loaders[\"train\"], desc=f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "        for step, (vols, labels) in enumerate(batch_iter, start=1):\n",
    "            vols = vols.to(device)\n",
    "            labels = labels.to(device)\n",
    "            logits = model(vols)\n",
    "            loss = criterion(logits, labels) / grad_accum_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if step % grad_accum_steps == 0 or step == len(loaders[\"train\"]):\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                grad_norms.append(float(grad_norm.item()))\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            train_losses.append(loss.item() * grad_accum_steps)\n",
    "            batch_iter.set_postfix({\n",
    "                \"loss\": f\"{loss.item() * grad_accum_steps:.4f}\",\n",
    "                \"grad_norm\": f\"{grad_norms[-1]:.3f}\" if grad_norms else \"-\",\n",
    "            })\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Evaluation\n",
    "        val_metrics = evaluate(model, loaders[\"val\"], device, num_classes=num_classes)\n",
    "        val_cm = compute_confusion_matrix(model, loaders[\"val\"], device, num_classes=num_classes)\n",
    "\n",
    "        train_metrics = {\n",
    "            \"loss\": float(np.mean(train_losses)),\n",
    "            \"grad_norm_avg\": float(np.mean(grad_norms)) if grad_norms else float(\"nan\"),\n",
    "            \"grad_norm_max\": float(np.max(grad_norms)) if grad_norms else float(\"nan\"),\n",
    "        }\n",
    "        history[\"train\"].append(train_metrics)\n",
    "        history[\"val\"].append(val_metrics)\n",
    "        history[\"val_confusion\"].append(val_cm.tolist())\n",
    "\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        print(f\"  Train Loss: {train_metrics['loss']:.4f}\")\n",
    "        print(\n",
    "            f\"  Grad Norm - Avg: {train_metrics['grad_norm_avg']:.3f} | \"\n",
    "            f\"Max: {train_metrics['grad_norm_max']:.3f}\"\n",
    "        )\n",
    "        print(f\"  Val - Acc: {val_metrics['accuracy']:.4f} | F1(macro): {val_metrics['f1_macro']:.4f} | AUC(macro): {val_metrics['auc_macro']:.4f}\")\n",
    "        per_class_f1 = \" | \".join(\n",
    "            f\"{name}: {val_metrics.get(f'f1_{name}', float('nan')):.4f}\"\n",
    "            for name in Config.CLASS_NAMES\n",
    "        )\n",
    "        print(f\"  Per-class F1 - {per_class_f1}\")\n",
    "        print(f\"  Confusion Matrix:\\n{val_cm}\")\n",
    "\n",
    "        if val_metrics[\"f1_macro\"] > best_f1:\n",
    "            best_f1 = val_metrics[\"f1_macro\"]\n",
    "            patience = 0\n",
    "            checkpoint = {\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'best_f1': best_f1,\n",
    "                'val_metrics': val_metrics,\n",
    "                'config': {\n",
    "                    'seed': Config.SEED,\n",
    "                    'batch_size': Config.BATCH_SIZE,\n",
    "                    'effective_batch_size': Config.BATCH_SIZE * Config.GRAD_ACCUM_STEPS,\n",
    "                    'grad_accum_steps': Config.GRAD_ACCUM_STEPS,\n",
    "                    'lr': Config.LR,\n",
    "                    'epochs': Config.EPOCHS,\n",
    "                    'early_stop_patience': Config.EARLY_STOP_PATIENCE,\n",
    "                    'dropout': Config.DROPOUT,\n",
    "                    'model_name': Config.MODEL_NAME,\n",
    "                    'num_classes': Config.NUM_CLASSES,\n",
    "                    'class_names': Config.CLASS_NAMES,\n",
    "                    'target_shape': Config.TARGET_SHAPE,\n",
    "                    'test_size': Config.TEST_SIZE,\n",
    "                    'val_size': Config.VAL_SIZE,\n",
    "                },\n",
    "                'random_state': {\n",
    "                    'python': random.getstate(),\n",
    "                    'numpy': np.random.get_state(),\n",
    "                    'torch': torch.get_rng_state(),\n",
    "                    'cuda': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,\n",
    "                },\n",
    "                'history': history,\n",
    "            }\n",
    "            torch.save(checkpoint, output_dir / \"best_model.pth\")\n",
    "            print(f\"  âœ“ New best model saved (F1 macro: {best_f1:.4f})\")\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        if patience >= early_stop_patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    final_checkpoint = {\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'best_f1': best_f1,\n",
    "        'config': {\n",
    "            'seed': Config.SEED,\n",
    "            'batch_size': Config.BATCH_SIZE,\n",
    "            'effective_batch_size': Config.BATCH_SIZE * Config.GRAD_ACCUM_STEPS,\n",
    "            'grad_accum_steps': Config.GRAD_ACCUM_STEPS,\n",
    "            'lr': Config.LR,\n",
    "            'epochs': Config.EPOCHS,\n",
    "            'early_stop_patience': Config.EARLY_STOP_PATIENCE,\n",
    "            'dropout': Config.DROPOUT,\n",
    "            'model_name': Config.MODEL_NAME,\n",
    "            'num_classes': Config.NUM_CLASSES,\n",
    "            'class_names': Config.CLASS_NAMES,\n",
    "            'target_shape': Config.TARGET_SHAPE,\n",
    "            'test_size': Config.TEST_SIZE,\n",
    "            'val_size': Config.VAL_SIZE,\n",
    "        },\n",
    "        'random_state': {\n",
    "            'python': random.getstate(),\n",
    "            'numpy': np.random.get_state(),\n",
    "            'torch': torch.get_rng_state(),\n",
    "            'cuda': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,\n",
    "        },\n",
    "        'history': history,\n",
    "    }\n",
    "    torch.save(final_checkpoint, output_dir / \"final_model.pth\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_header",
   "metadata": {
    "id": "run_header"
   },
   "source": [
    "---\n",
    "## ğŸš€ í•™ìŠµ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_setup",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16658,
     "status": "ok",
     "timestamp": 1771496565527,
     "user": {
      "displayName": "KDT7",
      "userId": "17761287236506147872"
     },
     "user_tz": -540
    },
    "id": "run_setup",
    "outputId": "34a827ed-5cd5-4b67-c8ba-177cc2aff063"
   },
   "outputs": [],
   "source": [
    "# ì‹œë“œ ê³ ì •\n",
    "torch.manual_seed(Config.SEED)\n",
    "np.random.seed(Config.SEED)\n",
    "\n",
    "# ì¸ë±ìŠ¤/ë¶„í• \n",
    "df = build_index(Config.DATA_DIR, Config.METADATA_PATH)\n",
    "splits = split_by_subject(df, SplitConfig())\n",
    "\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Train: {len(splits['train'])} | Val: {len(splits['val'])} | Test: {len(splits['test'])}\")\n",
    "print(f\"Class distribution (train): {splits['train']['label'].value_counts().to_dict()}\")\n",
    "\n",
    "# Split ì €ì¥\n",
    "Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "for split_name, split_df in splits.items():\n",
    "    split_df.to_csv(Config.OUTPUT_DIR / f\"{split_name}_split.csv\", index=False)\n",
    "\n",
    "# ë¡œë”\n",
    "loaders = build_dataloaders(\n",
    "    splits,\n",
    "    target_shape=Config.TARGET_SHAPE,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    num_workers=Config.NUM_WORKERS,\n",
    "    use_weighted_sampler=Config.USE_WEIGHTED_SAMPLER,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_roi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4700,
     "status": "ok",
     "timestamp": 1771496570233,
     "user": {
      "displayName": "KDT7",
      "userId": "17761287236506147872"
     },
     "user_tz": -540
    },
    "id": "visualize_roi",
    "outputId": "52775ff0-c356-4bee-a618-54c356a5f708"
   },
   "outputs": [],
   "source": [
    "# ğŸ¯ ROI ê°€ì¤‘ì¹˜ ë§µ ì‹œê°í™”\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if ROIConfig.USE_ROI_ATTENTION:\n",
    "    wm = create_roi_weight_map(\n",
    "        Config.TARGET_SHAPE,\n",
    "        ROIConfig.ROI_DEFINITIONS,\n",
    "        ROIConfig.BASE_WEIGHT\n",
    "    )[0, 0].numpy()\n",
    "\n",
    "    # ìƒ˜í”Œ 1ê°œ ë¡œë“œ\n",
    "    sample_ds = NiftiDataset(\n",
    "        splits['train'].iloc[:1].reset_index(drop=True),\n",
    "        target_shape=Config.TARGET_SHAPE,\n",
    "        augment=False\n",
    "    )\n",
    "    vol, label = sample_ds[0]\n",
    "    vol_np = vol[0].numpy()\n",
    "\n",
    "    d, h, w = vol_np.shape\n",
    "\n",
    "    # ì£¼ìš” ROI ìœ„ì¹˜ì—ì„œ ìŠ¬ë¼ì´ìŠ¤\n",
    "    # í•´ë§ˆ ë†’ì´ (D=38), ì—”í† ë¼ì´ë‚  ë†’ì´ (D=32), í›„ëŒ€ìƒí”¼ì§ˆ ë†’ì´ (D=55)\n",
    "    slices_d = [48, 54, 48]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    # ìƒë‹¨: Axial slices at key ROI depths\n",
    "    for i, d_idx in enumerate(slices_d):\n",
    "        axes[0, i].imshow(vol_np[d_idx, :, :], cmap='gray')\n",
    "        im = axes[0, i].imshow(wm[d_idx, :, :], cmap='hot', alpha=0.5, vmin=1.0, vmax=2.0)\n",
    "        axes[0, i].set_title(f'Axial D={d_idx}')\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "    # í•˜ë‹¨: Coronal, Sagittal at center\n",
    "    axes[1, 0].imshow(vol_np[:, h//2, :], cmap='gray')\n",
    "    axes[1, 0].imshow(wm[:, h//2, :], cmap='hot', alpha=0.5, vmin=1.0, vmax=2.0)\n",
    "    axes[1, 0].set_title(f'Coronal H={h//2}')\n",
    "    axes[1, 0].axis('off')\n",
    "\n",
    "    axes[1, 1].imshow(vol_np[:, :, 30], cmap='gray')  # Left hemisphere\n",
    "    axes[1, 1].imshow(wm[:, :, 30], cmap='hot', alpha=0.5, vmin=1.0, vmax=2.0)\n",
    "    axes[1, 1].set_title('Sagittal W=30 (Left)')\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "    axes[1, 2].imshow(vol_np[:, :, 66], cmap='gray')  # Right hemisphere\n",
    "    axes[1, 2].imshow(wm[:, :, 66], cmap='hot', alpha=0.5, vmin=1.0, vmax=2.0)\n",
    "    axes[1, 2].set_title('Sagittal W=66 (Right)')\n",
    "    axes[1, 2].axis('off')\n",
    "\n",
    "    plt.colorbar(im, ax=axes.ravel().tolist(), fraction=0.02, pad=0.02, label='Weight')\n",
    "    plt.suptitle('ROI Weight Map Overlay', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Config.OUTPUT_DIR / 'roi_weight_map.png', dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('ROI attention disabled.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build_model",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1771496570632,
     "user": {
      "displayName": "KDT7",
      "userId": "17761287236506147872"
     },
     "user_tz": -540
    },
    "id": "build_model",
    "outputId": "e68e3190-29c9-4aa9-c41f-079a92b085b2"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ìƒì„±\n",
    "roi_attention = build_roi_attention(Config, ROIConfig)\n",
    "\n",
    "model_name = str(Config.MODEL_NAME).lower().replace(\" \", \"\")\n",
    "\n",
    "if model_name == \"attention\":\n",
    "    model = MCIClassifierWithAttention(\n",
    "        num_classes=Config.NUM_CLASSES,\n",
    "        dropout=Config.DROPOUT,\n",
    "        roi_attention=roi_attention\n",
    "    )\n",
    "elif model_name == \"transformer\":\n",
    "    model = MCIClassifierCNNTransformer(\n",
    "        num_classes=Config.NUM_CLASSES,\n",
    "        dropout=Config.DROPOUT,\n",
    "        roi_attention=roi_attention\n",
    "    )\n",
    "elif model_name == \"lstm\":\n",
    "    model = MCIClassifierCNNLSTM(\n",
    "        num_classes=Config.NUM_CLASSES,\n",
    "        dropout=Config.DROPOUT,\n",
    "        roi_attention=roi_attention\n",
    "    )\n",
    "elif model_name == \"gru\":\n",
    "    model = MCIClassifierCNNGRU(\n",
    "        num_classes=Config.NUM_CLASSES,\n",
    "        dropout=Config.DROPOUT,\n",
    "        roi_attention=roi_attention\n",
    "    )\n",
    "elif model_name == \"translstm\":\n",
    "    model = MCIClassifierTransLSTM(\n",
    "        num_classes=Config.NUM_CLASSES,\n",
    "        dropout=Config.DROPOUT,\n",
    "        roi_attention=roi_attention\n",
    "    )\n",
    "elif model_name == \"transgru\":\n",
    "    model = MCIClassifierTransGRU(\n",
    "        num_classes=Config.NUM_CLASSES,\n",
    "        dropout=Config.DROPOUT,\n",
    "        roi_attention=roi_attention\n",
    "    )\n",
    "elif model_name == \"basic\":\n",
    "    model = MCIClassifier3D(\n",
    "        num_classes=Config.NUM_CLASSES,\n",
    "        dropout=Config.DROPOUT,\n",
    "        roi_attention=roi_attention\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported MODEL_NAME: {Config.MODEL_NAME}\")\n",
    "\n",
    "model.to(Config.DEVICE)\n",
    "\n",
    "# íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model: {Config.MODEL_NAME}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"ROI Attention: {'Enabled' if roi_attention else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_model",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì‹¤í–‰\n",
    "history = train(\n",
    "    model=model,\n",
    "    loaders=loaders,\n",
    "    device=Config.DEVICE,\n",
    "    epochs=Config.EPOCHS,\n",
    "    lr=Config.LR,\n",
    "    output_dir=Config.OUTPUT_DIR,\n",
    "    early_stop_patience=Config.EARLY_STOP_PATIENCE,\n",
    "    grad_accum_steps=Config.GRAD_ACCUM_STEPS,\n",
    "    num_classes=Config.NUM_CLASSES,\n",
    ")\n",
    "\n",
    "# History ì €ì¥\n",
    "with open(Config.OUTPUT_DIR / \"history.json\", \"w\") as f:\n",
    "    json.dump(history, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test_eval",
   "metadata": {
    "id": "test_eval"
   },
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ í‰ê°€\n",
    "checkpoint = torch.load(Config.OUTPUT_DIR / \"best_model.pth\", weights_only=False)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "test_metrics = evaluate(model, loaders[\"test\"], Config.DEVICE, Config.NUM_CLASSES)\n",
    "test_cm = compute_confusion_matrix(model, loaders[\"test\"], Config.DEVICE, Config.NUM_CLASSES)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST RESULTS (MCI vs AD)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:        {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"F1 (macro):      {test_metrics['f1_macro']:.4f}\")\n",
    "print(f\"F1 (weighted):   {test_metrics['f1_weighted']:.4f}\")\n",
    "print(f\"Precision(macro):{test_metrics['precision_macro']:.4f}\")\n",
    "print(f\"Recall(macro):   {test_metrics['recall_macro']:.4f}\")\n",
    "print(f\"AUC (macro):     {test_metrics['auc_macro']:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Per-class metrics:\")\n",
    "for name in Config.CLASS_NAMES:\n",
    "    print(f\"  {name:>3} - Precision: {test_metrics[f'precision_{name}']:.4f} | \"\n",
    "          f\"Recall: {test_metrics[f'recall_{name}']:.4f} | \"\n",
    "          f\"F1: {test_metrics[f'f1_{name}']:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Confusion Matrix:\")\n",
    "print(f\"{'':>10} {'Pred CN':>8} {'Pred MCI':>9} {'Pred AD':>8}\")\n",
    "for i, name in enumerate(Config.CLASS_NAMES):\n",
    "    print(f\"True {name:>3}   {test_cm[i,0]:>8} {test_cm[i,1]:>9} {test_cm[i,2]:>8}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "with open(Config.OUTPUT_DIR / \"test_results.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"metrics\": test_metrics,\n",
    "        \"confusion_matrix\": test_cm.tolist()\n",
    "    }, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gradcam_header",
   "metadata": {
    "id": "gradcam_header"
   },
   "source": [
    "---\n",
    "## ğŸ” CAM ì‹œê°í™” (Original / Grad-CAM / Pixel-Gradient / PCG-CAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gradcam",
   "metadata": {
    "id": "gradcam"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "def _resolve_target_layer(model: nn.Module) -> nn.Module:\n",
    "    if hasattr(model, 'encoder'):\n",
    "        return model.encoder[-1].block[3]\n",
    "    if hasattr(model, 'block4'):\n",
    "        return model.block4.block[3]\n",
    "    if hasattr(model, 'layer4'):\n",
    "        return model.layer4\n",
    "    raise RuntimeError('Grad-CAM target layer not found')\n",
    "\n",
    "\n",
    "class GradCAM3D:\n",
    "    def __init__(self, model: nn.Module, target_layer: nn.Module) -> None:\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "        self._handles = []\n",
    "\n",
    "        def forward_hook(_, __, output):\n",
    "            self.activations = output\n",
    "\n",
    "        def backward_hook(_, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0]\n",
    "\n",
    "        self._handles.append(target_layer.register_forward_hook(forward_hook))\n",
    "        self._handles.append(target_layer.register_full_backward_hook(backward_hook))\n",
    "\n",
    "    def remove(self) -> None:\n",
    "        for h in self._handles:\n",
    "            h.remove()\n",
    "        self._handles = []\n",
    "\n",
    "    def __call__(self, x: torch.Tensor, class_idx: int | None = None) -> torch.Tensor:\n",
    "        self.model.zero_grad(set_to_none=True)\n",
    "        logits = self.model(x)\n",
    "        if class_idx is None:\n",
    "            class_idx = int(torch.argmax(logits, dim=1)[0])\n",
    "        score = logits[:, class_idx].sum()\n",
    "        score.backward()\n",
    "\n",
    "        weights = self.gradients.mean(dim=(2, 3, 4), keepdim=True)\n",
    "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
    "        cam = torch.relu(cam)\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-6)\n",
    "        return cam\n",
    "\n",
    "\n",
    "def compute_pixel_gradient(model: nn.Module, x: torch.Tensor, class_idx: int) -> np.ndarray:\n",
    "    \"\"\"Pixel-Gradient CAM: ì…ë ¥ í”½ì…€ì— ëŒ€í•œ ì¶œë ¥ gradient\"\"\"\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    x_input = x.clone().detach().requires_grad_(True)\n",
    "    logits = model(x_input)\n",
    "    score = logits[:, class_idx].sum()\n",
    "    score.backward()\n",
    "\n",
    "    grad = x_input.grad.detach().cpu().numpy()[0, 0]  # (D, H, W)\n",
    "    grad = np.abs(grad)\n",
    "    grad = grad - grad.min()\n",
    "    grad = grad / (grad.max() + 1e-6)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def compute_pcg_cam(grad_cam_np: np.ndarray, pixel_grad_np: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"PCG-CAM: Grad-CAM Ã— Pixel-Gradient (element-wise product)\"\"\"\n",
    "    pcg = grad_cam_np * pixel_grad_np\n",
    "    pcg = pcg - pcg.min()\n",
    "    pcg = pcg / (pcg.max() + 1e-6)\n",
    "    return pcg\n",
    "\n",
    "\n",
    "def compute_roi_anomaly_scores(\n",
    "    cam_map: np.ndarray,\n",
    "    roi_definitions: dict,\n",
    "    target_shape: tuple[int, int, int],\n",
    "    threshold: float = 0.3,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    ê° ROI ì˜ì—­ ë‚´ì—ì„œ CAM í™œì„±í™” ê¸°ë°˜ ì´ìƒì¹˜ ì ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤.\n",
    "\n",
    "    Returns:\n",
    "        ROIë³„ ì´ìƒì¹˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬)\n",
    "        - name: ROI ì´ë¦„\n",
    "        - description: ROI ì„¤ëª…\n",
    "        - mean_activation: ROI ë‚´ í‰ê·  í™œì„±í™” ê°’\n",
    "        - max_activation: ROI ë‚´ ìµœëŒ€ í™œì„±í™” ê°’\n",
    "        - anomaly_pct: ROI ë‚´ threshold ì´ìƒ í™œì„±í™”ëœ ë³µì…€ ë¹„ìœ¨(%)\n",
    "        - center: ROI ì¤‘ì‹¬ ì¢Œí‘œ\n",
    "    \"\"\"\n",
    "    D, H, W = target_shape\n",
    "    d = torch.arange(D).float()\n",
    "    h = torch.arange(H).float()\n",
    "    w = torch.arange(W).float()\n",
    "    dd, hh, ww = torch.meshgrid(d, h, w, indexing='ij')\n",
    "\n",
    "    results = []\n",
    "    for name, roi in roi_definitions.items():\n",
    "        center = roi[\"center\"]\n",
    "        sigma = roi[\"sigma\"]\n",
    "\n",
    "        # ROI ì˜ì—­ ë§ˆìŠ¤í¬: ê°€ìš°ì‹œì•ˆ > 0.5 (â‰ˆ1Ïƒ ë²”ìœ„)\n",
    "        dist_sq = ((dd - center[0])**2 + (hh - center[1])**2 + (ww - center[2])**2)\n",
    "        gaussian = torch.exp(-dist_sq / (2 * sigma**2))\n",
    "        mask = (gaussian > 0.5).numpy()\n",
    "\n",
    "        roi_voxels = cam_map[mask]\n",
    "        if len(roi_voxels) == 0:\n",
    "            continue\n",
    "\n",
    "        mean_act = float(roi_voxels.mean())\n",
    "        max_act = float(roi_voxels.max())\n",
    "        anomaly_pct = float((roi_voxels >= threshold).sum() / len(roi_voxels) * 100)\n",
    "\n",
    "        results.append({\n",
    "            \"name\": name,\n",
    "            \"description\": roi.get(\"description\", \"\"),\n",
    "            \"mean_activation\": mean_act,\n",
    "            \"max_activation\": max_act,\n",
    "            \"anomaly_pct\": anomaly_pct,\n",
    "            \"center\": center,\n",
    "            \"sigma\": sigma,\n",
    "        })\n",
    "\n",
    "    # í‰ê·  í™œì„±í™” ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬\n",
    "    results.sort(key=lambda x: x[\"mean_activation\"], reverse=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MCI ì˜ˆì¸¡ ì„±ê³µ ìƒ˜í”Œ ì°¾ê¸° (True=MCI, Pred=MCI)\n",
    "# ============================================================\n",
    "MCI_LABEL = Config.CLASS_NAMES.index(\"MCI\")\n",
    "\n",
    "model.eval()\n",
    "test_ds = NiftiDataset(splits[\"test\"], target_shape=Config.TARGET_SHAPE, augment=False)\n",
    "\n",
    "mci_correct_indices = []\n",
    "mci_correct_probs = []\n",
    "\n",
    "print(\"MCI ì˜ˆì¸¡ ì„±ê³µ ìƒ˜í”Œ íƒìƒ‰ ì¤‘...\")\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(test_ds)):\n",
    "        vol, label = test_ds[idx]\n",
    "        if label != MCI_LABEL:\n",
    "            continue\n",
    "        vol_input = vol.unsqueeze(0).to(Config.DEVICE)\n",
    "        logits = model(vol_input)\n",
    "        pred = int(torch.argmax(logits, dim=1)[0])\n",
    "        if pred == MCI_LABEL:\n",
    "            prob = float(torch.softmax(logits, dim=1)[0, MCI_LABEL].cpu().item())\n",
    "            mci_correct_indices.append(idx)\n",
    "            mci_correct_probs.append(prob)\n",
    "\n",
    "print(f\"MCI ì˜ˆì¸¡ ì„±ê³µ ìƒ˜í”Œ ìˆ˜: {len(mci_correct_indices)}\")\n",
    "\n",
    "if len(mci_correct_indices) == 0:\n",
    "    print(\"MCI ì˜ˆì¸¡ ì„±ê³µ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    best_idx = mci_correct_indices[np.argmax(mci_correct_probs)]\n",
    "    best_prob = max(mci_correct_probs)\n",
    "    subject_id = splits[\"test\"].iloc[best_idx][\"subject_id\"]\n",
    "    print(f\"ì„ íƒ ìƒ˜í”Œ - Subject: {subject_id}, MCI prob: {best_prob*100:.1f}%\")\n",
    "\n",
    "    vol, label = test_ds[best_idx]\n",
    "    vol_input = vol.unsqueeze(0).to(Config.DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(vol_input)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "    # --- 1) Grad-CAM ---\n",
    "    target_layer = _resolve_target_layer(model)\n",
    "    cam_gen = GradCAM3D(model, target_layer)\n",
    "    cam = cam_gen(vol_input, class_idx=MCI_LABEL)\n",
    "    cam_gen.remove()\n",
    "    cam_up = F.interpolate(cam, size=Config.TARGET_SHAPE, mode='trilinear', align_corners=False)\n",
    "    gradcam_np = cam_up.detach().cpu().numpy()[0, 0]\n",
    "\n",
    "    # --- 2) Pixel-Gradient CAM ---\n",
    "    pixgrad_np = compute_pixel_gradient(model, vol_input, class_idx=MCI_LABEL)\n",
    "\n",
    "    # --- 3) PCG-CAM ---\n",
    "    pcg_np = compute_pcg_cam(gradcam_np, pixgrad_np)\n",
    "\n",
    "    vol_np = vol_input.detach().cpu().numpy()[0, 0]\n",
    "\n",
    "    # ============================================================\n",
    "    # CAM ì¢…ë¥˜ë³„: ìµœëŒ€ í™œì„± ì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì›ë³¸ vs CAM ë‚˜ë€íˆ í‘œì‹œ\n",
    "    # ============================================================\n",
    "    def _plot_cam_max_activation(cam_name: str, cam_np: np.ndarray, vol_np: np.ndarray, max_pos: tuple[int, int, int]) -> None:\n",
    "        view_slices = [\n",
    "            (\"Axial\",    max_pos[0], lambda v, i: v[i, :, :]),\n",
    "            (\"Coronal\",  max_pos[1], lambda v, i: v[:, i, :]),\n",
    "            (\"Sagittal\", max_pos[2], lambda v, i: v[:, :, i]),\n",
    "        ]\n",
    "\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(10, 12))\n",
    "\n",
    "        for row, (view_name, slice_idx, slicer) in enumerate(view_slices):\n",
    "            base = slicer(vol_np, slice_idx)\n",
    "\n",
    "            ax0 = axes[row, 0]\n",
    "            ax0.imshow(base, cmap='gray')\n",
    "            if row == 0:\n",
    "                ax0.set_title(\"Original\", fontsize=12, fontweight='bold')\n",
    "            ax0.set_ylabel(f\"{view_name}\\\\n(slice={slice_idx})\", fontsize=11)\n",
    "            ax0.set_xticks([])\n",
    "            ax0.set_yticks([])\n",
    "\n",
    "            ax1 = axes[row, 1]\n",
    "            ax1.imshow(base, cmap='gray')\n",
    "            heat = slicer(cam_np, slice_idx)\n",
    "            im = ax1.imshow(heat, cmap='turbo', alpha=0.55, vmin=0.0, vmax=1.0)\n",
    "            if row == 0:\n",
    "                ax1.set_title(cam_name, fontsize=12, fontweight='bold')\n",
    "            ax1.set_xticks([])\n",
    "            ax1.set_yticks([])\n",
    "\n",
    "        # colorbar\n",
    "        cbar_ax = fig.add_axes([0.9, 0.15, 0.02, 0.7])\n",
    "        fig.colorbar(im, cax=cbar_ax, label='Activation')\n",
    "\n",
    "        prob_str = \" | \".join(f\"{n}: {probs[0, i].cpu().item()*100:.1f}%\" for i, n in enumerate(Config.CLASS_NAMES))\n",
    "        fig.suptitle(\n",
    "            f\"[MCI Correct] Subject: {subject_id}  |  True: MCI  |  Pred: MCI ({best_prob*100:.1f}%)\\\\n\"\n",
    "            f\"[{prob_str}]  |  {cam_name} Max activation (D={max_pos[0]}, H={max_pos[1]}, W={max_pos[2]})\",\n",
    "            y=0.98, fontsize=11\n",
    "        )\n",
    "        plt.subplots_adjust(wspace=0.02, hspace=0.08, right=0.88)\n",
    "        safe_name = re.sub(r\"[^a-z0-9]+\", \"_\", cam_name.lower()).strip(\"_\")\n",
    "        plt.savefig(Config.OUTPUT_DIR / f\"{safe_name}_max_activation.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    gradcam_max_pos = np.unravel_index(gradcam_np.argmax(), gradcam_np.shape)\n",
    "    pixgrad_max_pos = np.unravel_index(pixgrad_np.argmax(), pixgrad_np.shape)\n",
    "    pcg_max_pos = np.unravel_index(pcg_np.argmax(), pcg_np.shape)\n",
    "\n",
    "    cam_items = [\n",
    "        (\"Grad-CAM\", gradcam_np, gradcam_max_pos),\n",
    "        (\"Pixel-Gradient\", pixgrad_np, pixgrad_max_pos),\n",
    "        (\"PCG-CAM\", pcg_np, pcg_max_pos),\n",
    "    ]\n",
    "\n",
    "    for cam_name, cam_np, max_pos in cam_items:\n",
    "        print(f\"{cam_name} ìµœëŒ€ í™œì„± ì¢Œí‘œ (D, H, W): {max_pos}\")\n",
    "        _plot_cam_max_activation(cam_name, cam_np, vol_np, max_pos)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
